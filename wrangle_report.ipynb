{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Created: 07/03/2019"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report I will discuss the methodologies using during the wrangling phase of the WeRateDogs project. \n",
    "\n",
    "My wrangling efforts can be divided into 3 main subcategories: Gathering, Assessing and Cleaning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gathered sources from 3 different locations. \n",
    "\n",
    "The first source was programmatically downloaded from a url provided to me by Udacity. By using the .requests package the file was downloaded into the same directory as the jupyter notebook was launched. \n",
    "\n",
    "The second source was downloaded manually from one of Udacity’s workspaces. This was a relatively simple process whereby simply clicking on the started the download in my browser. The file was then moved to the correct directory. \n",
    "\n",
    "The last source was created by scraping information from WeRateDogs twitter feed using the twitter api library, tweepy. Once I had created my twitter account, the process for applying for a twitter developers account was followed. \n",
    "\n",
    "After authorising my developers accounts in the jupyter notebook using my secret consumer key, consumer secret, access token, access secret, I created an empty array and created a loop which ran through each tweet id provided to me in the second source. The twitter API produced an array in JSON format. This array when then dumped into a .txt file and then read by the pandas library. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the three sources was then assessed using a variety of manipulations including but not limited to, .head()  .sample()  .describe()    .info()\n",
    "\n",
    "Each three sources was visually and programmatically assessed and each issue that became apparent was then stored in a problems section that would be addressed in the final cleaning section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problems that were found whilst assessing the 3 dataframes were then tackled using a variety of methods. The cleaning process was broken down further into 3 subcategories. ‘Defining’ – which defined the problem being solved. \n",
    "‘Code’ – which showed the code being used to solve the problem and ‘Test’ – which tested the code to make sure it worked. \n",
    "\n",
    "Once the 3 tables were cleaned they were concatenated and this table was then assessed and cleaned again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the final master dataframe was deemed clean and tidy enough for analysis, it was saved as a csv in the same directory as the jupyter notebook was launched."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
